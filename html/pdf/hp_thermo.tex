In a quick review of thermodynamics, there is little hope of discussing the topic with any rigor, nor is this likely to be a good resource for learning it from scratch.  Instead, these pages are inteded to be a quick reference for the properties and how they might be used.

\section{Properties}
One way to discuss a substance's properties is as a set of descriptors for the substance at a particular thermodynamic state.  Broadly speaking, this might even include even qualitative descriptors like color and smell, but when we are talking about thermodynamics we are typically talking about different ways of describing the arrangement and motion of the atoms that make up the substance.  It makes good sense to do that in such a way that is immediately useful for whatever application we have in mind.  If it is important to calculate how long water takes to cool, internal energy may be an important tool, but if one is designing a steam turbine, then enthalpy and entropy may be quite important.

\subsection{Internal Energy}
Internal energy describes the amount of thermal energy stored in nuclear, atomic, molecular, and translational forces with units $kJ/kg$.  If you had to squeeze two atoms into each other in order to get them to stick together in a molecule; if you had to whack a molecule to get its atoms vibrating like little masses on springs; if you gave a molecule a push to get it moving around in space; you increased a substance's internal energy.  It is probably the most intuitively useful of the endless array of thermodynamic properties because it relates all of our hard-earned intuition about the macro-mechanical world (balls banging into walls and carts sliding down inclines) to the tiny world inside of substances.

Internal energy is often denoted with a $u$, but PYro uses $e$.  This notation associates it with its name in most languages using the Latin alphabet (English, German, French, Spanish, Portuguese, Italian, Afrikaans, but apparently not Welsh) and sets it apart in applications that reserve $u$ for velocity and $v$ for volume.  If you don't like it, just define your own \verb|.u()| method equal to \verb|.e()|!  We'll understand.

Because internal energy is the combination of so many different expressions of energy, it can get pretty complicated depending on the substance.  In version \version{} of PYro, we focus on substances in a gas phase, so let's start there.  The molecules that make up a gas are untethered to one another and fly about freely.  To learn more, ask your friendly neighborhood librarian for a book on the kinetic theory of gases.  In a nutshell, it will say that the thing we have long understood to be a gas's temperature is really the kinetic energy of molecules banging into surfaces.  The thing we understood to be pressure is just the forces due to the impact of the same particles.  Because the molecules are so very tiny and collide so very often, we do not perceive their collisions as individual events, but as a continuum of force and heat.

The interesting thing is that if temperature is a molecule's translational kinetic energy, then very simple molecules (monatomic Argon, for example) will rarely store energy as anything else, so the energy and the temperature will quite simply be proportional.  However, more complicated molecules can spin and vibrate in addition to flying around, so there will be a tendency for energy to be wicked away from what we recognize as temperature and start showing up in other forms.  As a result, these substances will tend to have more complicated relationships between temperature, pressure, and internal energy; hence the motivation for software like PYro.

In general, internal energy has a complicated relationship with temperature and pressure that can be experimentally determined (using calorimetry).  However, many gases behave in a way that makes the job of determining internal energy quite a bit easier.  We can understand why by beginning with a seemingly unrelated question: if pressure is the force due to impact (determined by momentum), and temperature is the average kinetic energy of molecules, how is it possible to independently vary pressure and temperature?  After all, molecules with a prescribed kinetic energy also have a prescribed momentum.  The answer is that nobody ever said anything about the number of molecules.  Temperature is a property that describes the average behavior of each molecule, but pressure is an aggregate force due to all the molecules in the vicinity of a surface.  More molecules means more force means more pressure.  If it is possible to cram more molecules together without affecting temperature, then it may also be possible to do the same without affecting internal energy!

When a gas behaves such that internal energy is not a function of pressure, it is said to be an \emph{ideal gas}.  We also get to use the familiar formula,
\begin{align}
p = \rho R T\label{eqn:iglaw}
\end{align}
to relate pressure, density, and temperature.  In fact, if you look closely at the ideal gas law in equation \ref{eqn:iglaw}, you can even see the idea living in the math.  For a given temperature, pressure can be increased by increasing the density of the gas.  For extensibility, PYro accepts pressure as an argument to the property functions for ideal gas data types (\verb|igtab| and \verb|igfit|), but it will only rarely be used (see entropy).

As of version \version{}, PYro does not install with multiphase data.  That will probably change soon, so it's worth talking about what happens when substances stop being gases and start being other things.

When a substance changes phase, the remarkable shift in character of the substance represents a fundamental change in how the molecules are arranged.  Solids have tightly spaced molecules that are in intimate contact with one another; in sharp contrast to gases.  Similarly, liquids also exhibit strong intermolecular forces that will tend to maintain their volume (like solids), but not their shape (not like solids).  As a result, pressure in these substances usually plays only a minor role (if any) for determining internal energy.  

What is very interesting is the intense quantity of energy released when substances shift from gas-to-liquid-to-solid.  Called \emph{latent} heat, this energy is what gets released when free-flying gas molecules whack into a liquid, shed their momentum, and are trapped as a sluggish liquid.  Latent heat shows up in internal energy as an incredibly mathematically inconvenient step change in internal energy when we move between the phases.  PYro doesn't include it now, but it will one day.

\subsection{Density, Ideal Gas Constant, and Molecular Weight}
Density describes the amount of something per unit volume in units kg/m$^3$.  Solids and liquids can compress and expand a little, but for the most part they are quite good at maintaining their density.  Gases, on the other hand are quite squishy.  When they behave ideally, we get to use Equation \ref{eqn:iglaw} to relate temperature and pressure to the density.  Buried in that relationship are the complicated ideas of gas kinetics described above.

It is important to remember that $R$ in Equation \ref{eqn:iglaw} is NOT 8.314kJ/kmol/K.  It has been scaled by the mass, $M$, of each molecule, so that
\begin{align}
R = \frac{\overline{R}}{mw}.
\end{align}
$M$ is often reported without units, because it is adapted to different unit systems by means of a redefinition of $moles$.  PYro uses $kg$ for mass, so it assumes $kmol$ as the standard unit for molecular count.

\subsection{Enthalpy}
Enthalpy is an example of the thermodynamic properties defined for convenience of application.  For a gas happily bouncing about in the confines of a container, enthalpy will do us little good, but when a gas is flowing and pushing on things, we need to account for both the internal energy and the mechanical work done by the flow.  Put another way, the act of moving a fluid in bulk from one pressure to another implies work just like moving electrons from one voltage to another or weights from one height to another.

Enthalpy is defined as
\begin{align}
h = e + \frac{p}{\rho},
\end{align}
and shares the kJ/kg units of its internal energy parent.  But, from whence does it come?

Consider a steady flow through a volume in space.  If the energy in the volume is to be constant, then the rate of accumulation of energy in that volume must be zero.  Energy will flow into the volume with individual particles carrying their own internal energy and the motion of materials under pressure will communicate work through fluid power.  If we imagine a tiny surface area, $\mathrm{d}S$, with an outward-facing unit surface normal, $\vec{n}$, the rate at which energy is carried out by bulk flow is $\rho e \vec{u} \cdot \vec{n} \mathrm{d}S$.  The rate at which the flow communicates fluid power through the small surface is $p \vec{u}\cdot\vec{n} \mathrm{d}S$.  A sum of those contributors over the entire surface must be zero.
\begin{align}
\oiint_S \rho e \vec{u}\cdot \vec{n} \mathrm{d}S + \oiint_S p \vec{u}\cdot \vec{n} \mathrm{d}S = 0
\end{align}
It only takes a little manipulation to see why enthalpy is so important.
\begin{align}
\oiint_S \rho \left(e + \frac{p}{\rho}\right) \vec{u}\cdot \vec{n} \mathrm{d}S = 0
\end{align}

In a twist that can only be described as counter-intuitive, if the substance is an ideal gas, enthalpy enjoys the same insensitivity to pressure as its internal energy cousin.  When we substitute the ideal gas law,
\begin{align}
h = e(T) + RT.
\end{align}

\subsection{Specific Heats}
When talking about internal energy, we noticed that substances store internal energy in lots of ways, and that only some of it manifests in what we would recognize as temperature.  It is extremely common to need to know how much energy it takes to change a substance's temperature.  Therefore, we define \emph{specific heat},
\begin{align}
c = \frac{\delta q}{\mathrm{d} T},\label{eqn:cdef}
\end{align}
where $\delta q$ is a small quantity of heat added to the substance per mass (kJ/kg), and $\mathrm{d} T$ is the resulting tiny rise in temperature.  The use of $\delta q$ (as opposed to $\mathrm{d} q$) is the result of an old debate.  The argument goes that it's because $q$ isn't a property of the substance.  Don't worry about it.

For complicated substances, hotter molecules may tend to gyrate around differently when they are cool, so different fractions of their energy will appear as temperature.  As a result $c$ can be quite a strong function of temperature.  However, nice simple substances might exhibit nearly constant $c$ over a wide range of temperatures.  If such a simple substance is a gas, it is called a \emph{perfect gas}.

The internal energy of an ideal gas doesn't depend on pressure, but letting a gas expand to a lower pressure will definitely cool it.  The first law tells us why; the fluid is doing work, and that energy had to come from somewhere.  That leads us to a conundrum when we are talking about the specific heat of gases; what is happening to the pressure?

The general definition for specific heat offered by \ref{eqn:cdef} simply isn't specific enough because it doesn't tell us what is happening to the pressure or volume of the substance while we are changing the temperature.  That may not be intuitive, so let's illustrate the idea with two examples.

First, imagine we want to measure the specific heat of a substance while holding its volume constant.  When there is no motion at the substance's boundaries no work is done, so the first law is quite simple
\begin{align}
\delta q &= \mathrm{d} e\nonumber\\
 &= \left(\frac{\partial e}{\partial T}\right)_{v=\mathrm{const}} \mathrm{d} T
\end{align}
In other words, the specific heat measured at a constant volume tells us the fraction of internal energy that we actually observe as temperature.  Of course, if the substance is an ideal gas, then it doesn't matter how we perform the partial derivative because internal energy is ONLY a function of temperature.

What if we repeated the same measurement, but while holding pressure constant?  This time, the substance will need to expand to prevent the pressure from increasing as the material is heated.  In solids and liquids, this effect is nearly irrelevant, but in gases, it is really very important.
\begin{align}
\delta q &= \mathrm{d} e + p \mathrm{d} v\nonumber\\
 &= \left(\frac{\partial e}{\partial T} + p\frac{\partial v}{\partial T} \right)_{p=\mathrm{const}} \mathrm{d} T\nonumber\\
 &= \left(\frac{\partial e}{\partial T} + \frac{\partial p/\rho}{\partial T} \right)_{p=\mathrm{const}} \mathrm{d} T\nonumber\\
 &= \left(\frac{\partial h}{\partial T}\right)_{p=\mathrm{const}} \mathrm{d} T
\end{align}
Here, $v$ is the specific volume or volume per unit mass.  Note that if the substance is an ideal gas it is irrelevant to the partial derivative whether pressure is held constant.

The idea is that adding heat to a gas can go into making it hotter or making it expand.  Usually, it does both, but there is no way to know how unless the process is well defined.  Therefore, the specific heats of gases are commonly reported as both constant-volume and constant-pressure specific heats,
\begin{align}
c_v &= \left(\frac{\partial e}{\partial T}\right)_{v=\mathrm{const}}\\
c_p &= \left(\frac{\partial h}{\partial T}\right)_{p=\mathrm{const}}.
\end{align}

To make things a little more convenient, their ratio is also commonly treated as a property,
\begin{align}
k = \frac{c_p}{c_v}.
\end{align}
The specific heat ratio is also often expressed as $\gamma$, but while typographically elegant, Greek letters just aren't very convenient at the command line.

For ideal gases, $c_v$ and $c_p$ enjoy a very simple relationship which can be derived from the definition of enthalpy,
\begin{align}
c_p &= \frac{\mathrm{d} h}{\mathrm{d} T}\nonumber\\
 &= \frac{\mathrm{d} (e + RT)}{\mathrm{d} T}\nonumber\\
 &= c_v + R
\end{align}
This teaches us that $R$ can be regarded as a kind of measure of the energy that goes into expanding a gas as it heats.

\subsection{Entropy}
Perhaps the most conceptually inaccessible of all the commonly used thermodynamic properties, entropy is the one that isn't like the others.  Energy and enthalpy all show up from thinking about conserving energy, but entropy becomes useful because of the second law.  In a nutshell, we need to enforce that heat flows from hot to cold and that useful engines reject waste heat.

The definition of entropy is probably best motivated by the Clausius inequality.  When we add or remove heat to a mechanism undergoing a ``cycle'' (like an engine) the temperature of the substance to which the heat is being cyclically added and removed really seems to matter.  The Clausius inequality tells us that when we follow the substance all the way through one cycle, unless
\begin{align}
\oint \frac{\delta q}{T} \le 0
\end{align}
the system will not be able to operate continuously.  People's best attempts to violate this rule just make the engine accumulate (or lose) energy until it stopped functioning or the rule was obeyed anyway.  For the best heat engine in the world,
\begin{align}
\oint \left(\frac{\delta q}{T}\right)_{\mathrm{int.rev.}} = 0
\end{align}
The subscript is an abbreviation for \emph{internally reversible}.  This describes a cycle built entirely of processes so beautifully executed that they can be driven forwards and backwards with the same net result.  In other words, friction, viscosity, leakage, and all the other nasty little realities of building a real system have been overcome.  As one might expect, that system has never actually been built.

Well, we used heat addition to define specific heat, so why can't we use it to define a new property?
\begin{align}
\mathrm{d}s &= \left(\frac{\delta q}{T}\right)_{\mathrm{int.rev.}}\label{eqn:sdef}
\end{align}
There's no need to stop there, because the substance's other properties tell us where the heat goes.
\begin{align}
T \mathrm{d}s &= \mathrm{d}e + p\mathrm{d}v\\
T \mathrm{d}s &= \mathrm{d}h - v\mathrm{d}p\label{eqn:dsdh}
\end{align}
We see here that enthalpy will have the same units as $R$ and specific heat, kJ/kg/K.

Equation \ref{eqn:dsdh} is where we find our source for integrating entropy for ideal gases.  When we have an ideal gas on our hands,
\begin{align}
\mathrm{d}s = \frac{c_p}{T} \mathrm{d}T - \frac{R}{p}\mathrm{d}p.
\end{align}
The entropy at standard pressure is 
\begin{align}
s^\circ (T) = \int \frac{c_p}{T} \mathrm{d}T
\end{align}
and the entropy at other pressures is
\begin{align}
s(T,p) = s^\circ (T) - R \mathrm{ln}\left(\frac{p}{p^\circ}\right)
\end{align}

If the substance is not an ideal gas, then we're stuck with Equations \ref{eqn:sdef} and \ref{eqn:dsdh} to figure out a substance's entropy.  On the other hand, when the substance is a perfect gas, things get very simple.
\begin{align}
s(T,p) = s_0 + c_p\mathrm{ln}\left(\frac{T}{T_0}\right) - R \mathrm{ln}\left(\frac{p}{p^\circ}\right)
\end{align}

\section{Mixtures}
As of version \version, PYro includes a mixture class and data for a handful of common gas mixtures, but it does not include tools for calculating the properties of mixtures whose compositions are changing (like in a reaction).  Instead, that is left to the user.  For the sake of better understanding the mixture class and helping users to do their own calculations with mixtures, a brief discussion on the properties of mixtures might be helpful.

\subsection{Defining a Mixture}
A mixture is defined by a set of constituent species and the amounts of each.  Those amounts might be specified as a list of masses, $M_k$, a list of mole counts, $N_k$, or in dimensionless quantities called the mass or mole fraction.  The mass fraction, $Y_k$, is the mass of a specific constituent species in ratio with the mass of the total mixture.
\begin{align}
Y_k &= \frac{M_k}{\sum_i M_i}\nonumber\\
 &= \frac{M_k}{M}
\end{align}
Similarly, the mole fraction is the ratio of the number of moles of a species in ratio with the total mole count.
\begin{align}
X_k &= \frac{N_k}{N}
\end{align}

As one might expect, they are related.  Given a mixture's mole fractions,
\begin{align}
{Y_k}^{-1} &= \frac{M}{M_k} = \frac{\sum_i mw_i N_i}{mw_k N_k}\nonumber\\
 &= (mw_k X_k)^{-1} \sum_i mw_i X_i
\end{align}
and given the same mixture's mass fractions,
\begin{align}
{X_k}^{-1} &= \frac{N}{N_k} = \frac{\sum_i M_i/mw_i}{M_k/mw_k}\nonumber\\
 &= \left(\frac{Y_k}{mw_k}\right)^{-1} \sum_i \frac{Y_i}{mw_i}
\end{align}

\subsection{Mass-Based Properties}
The internal energy, entropy, and enthalpy of a mixture can be found by summing the total extensive properties of its constituents and dividing by the total mass of the fluid.  What does that mean?  Consider the enthalpy of a mixture.
\begin{align}
h = \frac{\sum_k h_k M_k}{\sum_k M_k}
\end{align}
A little bit of algebra reveals that
\begin{align}
h = \sum_k h_k Y_k
\end{align}
where $Y_k$ is the ``mass fraction'' of each component, $Y_k = M_k/M$.

\subsection{Volumetric Properties}
At first glance, it is less obvious how to proceed for properties that are volume-based (with units ``per-m$^3$'').  Consider the density of a mixture of ideal gases.  It is the total mass of the substance divided by the total volume it occupies.  A quick look at the algebra can fool us into believing that we only need calculate the densities at the same temperature and pressure and sum them, but this gives us an unrealistically huge answer.  Why?  If we calculate the density of each constituent using the pressure we measured from the gas as a whole, we are drastically over-estimating the number of molecules of each.
\begin{align}
\rho(T,p) &= \frac{\sum_k M_k}{V} \ne \sum_k \rho_k(T,p)
\end{align}

Let's show two ways to approach this problem with the same result.  While the constituent gases may all exhibit the same temperature, each of them will only contribute to a fraction of the pressure.  Called the \emph{partial pressure}, it is the fraction of the pressure that is contributed by each constituent, the sum of which is what we actually measure.  It turns out that the partial pressure of each component is just the mole fraction, $X_k$, times the total pressure, $p$.
\begin{align}
\rho(T,p) &= \frac{\sum_k M_k}{V} \nonumber\\
 &= \sum_k \rho_k(T,p X_k)
\end{align}

Similarly, we could have used an algebraically motivated argument,
\begin{align}
\rho(T,p) &= \sum_k \frac{M_k}{V_k}\frac{V_k}{V}\nonumber\\
 &= \sum_k \rho_k(T,p) X_k
\end{align}
Note that for an ideal gas, these results are actually identical.  It is because at their core, these are the same argument.  They both suppose that density and pressure at a given temperature are just proportional to the number of molecules bouncing around, so the portion of a volume occupied by a gas is too.  Happily, this thinking extends to ideal solutions of liquids as well.

Unfortunately, as things get less ``ideal,'' the interactions in mixtures can become more complicated, and a specialized model might be called for.  PYro's \verb|mixture| class relies on the ideal solution and ideal gas assumption to calculate densities, so beware.

\subsection{Mole-Based Properties}
The same rule applies to mole-based (with units ``per-kmole'') properties like molecular weight.  The molecular weight (or mass) is the total mass of a substance in ratio with the number of moles present.
\begin{align}
mw &= \frac{\sum_k M_k}{\sum_k N_k} = \sum_k \frac{mw_k N_k}{N}\nonumber\\
 &= \sum_k mw_k X_k
\end{align}

We therefore have two classes of properties; those that behave as weighted averages of the masses present and those that behave as weighted averages of the moles present.  Enthalpy, entropy, and specific heats belong to the former, and density and molecular weight belong to the latter.
