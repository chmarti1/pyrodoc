\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}

\def\pm{PYroMat}

\title{The ITER1 and HYBRID1 iteration algorithms used in the PYroMat package}
\author{Christopher R. Martin\\Assistant Professor of Mechanical Engineering\\Penn State University}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

In multi-phase substances, the general equation of state for the evaluation of thermodynamic properties of substances is almost always of the form, $A(T,\rho)$; the Helmholtz free energy expressed as a function of temperature and density.  Ideal gases are almost always defined in terms of $c_p(T)$; where $c_p$ is constant-pressure specific heat.  For an ideal gas, enthalpy and specific heats do not depend on pressure, but entropy's pressure dependency is straightforward, so ideal gas properties are usually evaluated in terms of temperature and pressure instead of temperature and density.

In general, all of the substances modeled by \pm may be thought of as having properties that are a function of any two of $T$, $p$, and $\rho$.  As of version 2.1.0, virtually all property methods in \pm accept any two of these three as a constraint on the substance state.  For two-phase mixtures (which are possible in the multi-phase collection) ``quality'' or $x$ may be specified instead of one of these.

The underlying models on which \pm is based permit relatively straightforward algorithms for calculating properties in terms of these definitions of a substances ``state.''  There are, however, a number of applications where it is essential to calculate the numerical inverse of one of these properties.  As a simple example, the enthalpy of an ideal gas can be calculated directly from its temperature using the thermodynamic model for $h(T)$, but if enthalpy is known, an analytical expression for temperature is not available; numerical iteration is necessary.

\subsection{Formulation}

The generic problem of property inversion is two-dimensional.  There are two arguments, to any property evaluation, so, in theory, specifying any two properties should be sufficient to specify the arguments.  In this way, specifying enthalpy and entropy simultaneously should be enough to determine the state.  

Like all ideas, this one has its limits too, though.  For examples, liquids are so incompressible, that specifying their density doesn't actually do much for our ability to accurately calculate a pressure given a temperature.  The uncertainty of such a calculation is huge.  This is sometimes called a ``numerically stiff'' problem, so that a tiny error in the target density would have a huge impact on the calculated pressure.  These situations must either be avoided by cautious use of iterative algorithms or mitigated with tremendous precision in the values provided.  

In general, there are two property functions that are defined by formulae that cannot be algebraically inverted.  In this case, the problem is of the form, \underline{given:} $y_1$ and $y_1$, \underline{find:} $x_1$ and $x_2$ so that
\begin{align}
y_1 &= f_1(x_1, x_2)\\
y_2 &= f_2(x_1, x_2).
\end{align}


\section{One-Dimensional Inversion}

Many of the available methods restrict these to a powerful subset of the generic problem where one of the two independent variables is known.  For example, $T_h()$ methods calculate temperature when entropy and either density or pressure is known.  In ideal gas models, enthalpy is calculated as a function of temperature only, $h(T)$, and in multi-phase models, it is calculated as a function of temperature and density, $h(T,\rho)$.  If density is known, this amounts to a one-dimensional inversion problem on temperature.

In these cases, the problem ammounts to \underline{given:} $y$, \underline{find:} $x$ such that
\begin{align}
y = f(x).
\end{align}

\subsection{Introduction to Newton-Rhapson iteration}

Simple Newton-Rhapson iteration is based on a linear approximation for the funciton, $f$, which assumes that its derivative is readily calculated.  For a guess, $x_k$, the value $y$ can be obtained by moving the guess a distance, $\Delta x$,
\begin{align}
y = f(x_k) + f'(x_k) \Delta x.
\end{align}

\begin{algorithm}
\caption{Simple Newton-Rhapson iteration, $y = f(x)$}\label{alg:1d:newton}
\begin{algorithmic}
\REQUIRE A function and its derivative: $f(x)$, $f'(x)$
\REQUIRE An initial guess: $x_0$
\REQUIRE A target value, $y$, and acceptable error, $\epsilon$
\STATE Initialize the current guess: $x_k \leftarrow x_0$
\STATE Evaluate the function: $y_k \leftarrow f(x_k)$, $y'_k \leftarrow f'(x_k)$
\WHILE{Error is large: $|y-f(x_k)| < \epsilon$}
\STATE Calculate the next guess: $x_k \leftarrow x_k + (y - y_k)/y'_k$
\STATE Evaluate the function: $y_k \leftarrow f(x_k)$, $y'_k \leftarrow f'(x_k)$
\ENDWHILE
\RETURN: $x_k$
\end{algorithmic}
\end{algorithm}

Figure \ref{fig:1d:newton} shows an example iteration step using Newton-Rhapson iteration.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/1d_newton}
\caption{An example iteration step using simple Newton-Rhapson iteration.}\label{fig:1d:newton}
\end{figure}

Newton iteration converges quickly on ``well mannered'' functions that are more or less parabolic in their shape, but there are significant problems when functions show more complicated shapes.

\subsection{ITER1: Modified Newton-Rhapson Iteration}

In many cases, there are hard boundaries above which no solution is possible.  These conditions often occur along with numerically stiff functions like the one shown in Figure \ref{fig:1d:newton:bounded}.  Here the funciton diverges rapidly as it approaches an asymptote.

In many of these cases, the locations of those asymptotes can be known even if the exact solution to the problem is not.  The ITER1 algorithm defined in \pm is a Newton-Rhapston solver modified to respect upper and lower bounds that limit the range over which an algorithm may stray.  If a guess leaves this range, the algorithm halves the step, $\Delta x$, until the new guess is back in range.

In Figure \ref{fig:1d:newton:bounded}, the first iteration produces a guess outside of the upper boundary, so $\Delta x$ is adjusted until the new guess is in-bounds again.  In this example, only one adjustment is necessary, but it could have continued many times.

\begin{figure}
\centering
\includegraphics[width = 0.8\linewidth]{figures/1d_newton_bounded}
\caption{An example of an iteration step using the ITER1 method.  The red box represents values beyond the legal solution boundary.}\label{fig:1d:newton:bounded}
\end{figure}

\begin{algorithm}
\caption{ITER1: Modified bounded Newton-Rhapson iteration, $y = f(x)$}\label{alg:1d:newton}
\begin{algorithmic}
\REQUIRE A function and its derivative: $f(x)$, $f'(x)$
\REQUIRE An initial guess: $x_0$
\REQUIRE A maximum and minimum boundary on $x$: $x_a < x <x_b$
\REQUIRE A target value, $y$, and acceptable error, $\epsilon$
\STATE Initialize the current guess: $x_k \leftarrow x_0$
\STATE Evaluate the function: $y_k \leftarrow f(x_k)$, $y'_k \leftarrow f'(x_k)$
\WHILE{Error is large: $|y-y_k| < \epsilon$}
\STATE Calculate a step size: $\Delta x \leftarrow (y - y_k)/y'_k$
\STATE Calculate the next guess: $x_{k+1} \leftarrow x_k + \Delta x$
\WHILE{$x_{k+1}$ is out of bounds: $x_{k+1} > x_b$ or $x_{k+1} < x_a$}
\STATE Halve the step size: $\Delta x \leftarrow \Delta x / 2$
\STATE Calculate the next guess: $x_{k+1} \leftarrow x_k + \Delta x$
\ENDWHILE
\STATE Shift the guesses: $x_{k+1} \leftarrow x_k$
\STATE Evaluate the function: $y_k \leftarrow f(x_k)$, $y'_k \leftarrow f'(x_k)$
\ENDWHILE
\RETURN $x_k$
\end{algorithmic}
\end{algorithm}

This method is quite robust in the sense that it will either converge to an answer or it will toil on infinitely.  Provided the limits on $x$ are well selected, it cannot diverge wildly like the guess in Figure \ref{fig:1d:newton:bounded}.

It should be emphasized, however, that Newton-Rhapson iteration is highly susceptible to infinite cycles when functions are shaped like Figure \ref{fig:1d:newton:cycle}.  This depicts an infinite cycle of guesses that will never converge to a solution.  It could be disregarded as an esoteric curiosity were it not precisely the type of failure that occurs when iterating on properties near phase changes (or near the critical point of a fluid).

\begin{figure}
\centering
\includegraphics[width = 0.8\linewidth]{figures/1d_newton_cycle}
\caption{An example of an endless cycle using bisection or ITER1.}\label{fig:1d:newton:cycle}
\end{figure}

For this reason, the ITER1 algorithm is relegated to the well behaved curves found in ideal gas properties.  A more sophisticated algorithm is needed for multi-phase properties.  In practice, ITER1 usually converges in three or four iterations to .001\% or better on ideal gas data.  There are some species that use the Shomate equation that have small discontinuities in their specific heat curves.  In extremely rare circumstances, this can cause strange numerical problems when the solution lies very near the discontinuity.

\subsection{An introduction to bisection iteration}

Given that failure to converge can be as bad as crashing, there is some incentive to implement a numerical routine that \emph{must} converge.  In one dimension, the bisection algorithm is an excellent choice when stability is valued over speed.

If upper and lower bounds, $x_a$ and $x_b$ are found so that $f(x_a) < y < f(x_b)$, then any continuous function must have a value somewhere between $x_a$ and $x_b$ such that $f(x) = y$.  If we were to divide the domain in half $x_c = (x_a+x_b)/2$ and evaluate the function there, its value will either be above or below $y$, so one of the two values could be replaced by $x_c$.  In this way, the domain between $x_a$ and $x_b$ is reliably cut in half with every iteration step, no matter how bizarrely $f(x)$ behaves.

This process just has to be repeated until the distance between the upper and lower bounds have shrunk to be so small that the numerical uncertainty is acceptable.  Figure \ref{fig:1d:bisection} shows three steps using this process.  The vertical space occupied by each blue box represents the shrinking uncertainty for the value of $f(x)$ and the horizontal space represents the shrinking uncertainty in $x$.

\begin{figure}
\centering
\includegraphics[width = 0.8\linewidth]{figures/1d_bisection}
\caption{An example of three iteration steps using the bisection algorithm.}\label{fig:1d:bisection}
\end{figure}

While Newton iteration might converge in only a few steps, bisection can take tens of steps depending on the ratio of the initial domain and the acceptable error range.  Since the domain is divided by two every time, it is easy to calculate the number of iterations to obtain a certain domain size.
\begin{align}
N = \log_2 \frac{|x_b - x_a|}{\epsilon}
\end{align}
If the initial domain were a temperature range of 0 to 1000 Kelvin, and the acceptable error were .01 Kelvin, 17 iterations are required.  If the acceptable error were tightened to .001 Kelvin, 20 iterations are required!

\subsection{HYBRID1}

It is difficult to walk away from guaranteed convergence even if bisection iteration is very slow.  It would be nice if there were some way to use a Newton-like algorithm that defaulted to a bisection algorithm when things go badly.  The trouble is how to figure out when things are going badly without adding numerically expensive checks.

Like bisection, HYBRID1 begins with upper and lower bounds, $x_a$ and $x_b$, which must bracket a solution.  Unlike bisection, HYRBID1 does not assume that all points in the range are equally likely to be solutions.  Newton iteration is used as much as possible, but (1) the two bounds must always bracket a solution, and (2) no guess should ever widen the bounds.

Each iteration step is allowed to select one of three candidate guesses.  As depicted in Figure \ref{fig:1d:hybrid}, two guesses are generated by performing a Newton iteration step at the upper and lower bounds, and a third bisects the domain.  The HYBRID1 algorithm selects the middle (or median) of the three.  

\begin{figure}
\centering
\includegraphics[width = 0.8\linewidth]{figures/1d_hybrid}
\caption{An example of a single iteration step using the HYBRID1 algorithm.}\label{fig:1d:hybrid}
\end{figure}

We may characterize each Newton iteration guess as lying in one of four segments of the $x$ domain:
\begin{enumerate}
\item Below the domain, $x < x_a$
\item In the lower half of the domain, $x_a < x < x_3$
\item In the upper half of the domain, $x_3 < x < x_b$
\item Above the domain, $x > x_b$
\end{enumerate}
That represents 16 possible combinations for where the Newton guesses could lie.

There are two cases where both Newton guesses lie in the same half of the domain.  This means our next guess will be guaranteed to shrink the domain, and because the two Newton iterations are in agreement, there is an excellent probability that the real solution lies nearby.  In this event, the median value will select the Newton iteration nearest the center of the domain - an excellent balance between wanting to shrink the domain as quickly as possible while still benefiting from the speed of Newton iteration.

There are eight cases in which the two Newton guesses lie on oposite sides of the domain center.  In all of these situations, Newton iteration seems to provide us no better insight than bisection, so we should fall back on the certainty that the domain will be reduced by half no matter what.  In this situation, the median value will be the bisection guess.

There are four cases where the two Newton guesses will be on the same side of the domain center, but one lies outside of the domain.  In these situation, the Newton iterations agree on the direction where the solution should be found, but the median value will automatically disregard the one that attempts to leave the domain.

The only degenerate cases are the two where both Newton iterations are outside the domain and on the same side.  This can be easily checked by verifying that the median value is inside the domain.  If not, the bisection guess should be used instead.

The function and its derivative are only evaluated at the selected candidate value; the other two are left alone.  This description of the process may lead one to imagine that each iteration step requires two function evaluations, but that is not so.  Just like the bisection algorithm, the appropriate upper or lower bound is replaced by the next guess so that the two bounds continue to bracket a solution.  The efficiency of this approach is realized by not discarding the Newton iteration already performed on the other bound.  Only one new function evaluation is necessary per step.

\begin{algorithm}
\caption{HYBRID1: Hybrid bisection and Newton iteration}\label{alg:hybrid1}
\begin{algorithmic}
\REQUIRE A function and its derivative: $f(x)$, $f'(x)$
\REQUIRE A maximum and minimum boundary on $x$: $x_a < x <x_b$
\REQUIRE An acceptable error in $x$, $\epsilon_x$
\REQUIRE A target value and acceptable error, $y$, $\epsilon_y$
\STATE Evaluate $f$ at the boundaries: 
\STATE $y_a \leftarrow f(x_a)$, $y'_a \leftarrow f'(x_a)$, $y_b \leftarrow f(x_b)$, $y'_b \leftarrow f'(x_b)$
\IF{$y_a > y_b$}
\STATE Swap $a$ and $b$: 
\STATE $x_a,y_a,y'_a \leftrightarrow x_b,y_b,y'_b$
\ENDIF
\IF{NOT $y_a < y < y_b$}
\RETURN ERROR: the solution may not be bracketed.
\ENDIF
\STATE Calculate three candidate guesses:
\STATE $x_1 \leftarrow x_a + (y-y_a)/y'_a$
\STATE $x_2 \leftarrow x_b + (y-y_b)/y'_b$
\STATE $x_3 \leftarrow (x_a + x_b)/2$
\WHILE{$|x_b-x_a|> \epsilon_x$}
\STATE Select a candidate: 
\STATE $x_c \leftarrow \mathrm{median}(x_1,x_2,x_3)$
\IF{NOT $x_a < x_c < x_b$}
\STATE Force bisection: 
\STATE $x_c \leftarrow x_3$
\ENDIF
\STATE Evaluate $f$: 
\STATE $y_c \leftarrow f(x_c)$, $y'_c\leftarrow f'(x_c)$
\IF{$|y_c - y| < \epsilon_y$}
\RETURN $x_c$
\ENDIF
\IF{$y_c < y$}
\STATE Replace the lower bound: 
\STATE $x_a \leftarrow x_c$, $y_a \leftarrow y_c$, $y'_a \leftarrow y'_c$
\STATE Calculate a new lower bound guess:
\STATE $x_1 \leftarrow x_a + (y - y_a)/y'_a$
\ELSE
\STATE Replace the upper bound:
\STATE $x_b \leftarrow x_c$, $y_b \leftarrow y_c$, $y'_b \leftarrow y'_c$
\STATE Calculate a new upper bound guess:
\STATE $x_2 \leftarrow x_b + (y - y_b)/y'_b$
\ENDIF
\STATE Calculate a new bisection guess:
\STATE $x_3 = (x_a + x_b)/2$
\RETURN $x_3$
\ENDWHILE

\end{algorithmic}
\end{algorithm}

The reader is encouraged to walk through the algorithm graphically using Figure \ref{fig:1d:hybrid}.  It should be apparent that the second iteration step will be a bisection step even though a second Newton iteration should be more expedient.  However, the third step arrives remarkably close to the solution.  In this way, the cautious approach employed in HYBRID1 carries a cost when employed on well behaved functions.

In practice, HYBRID1 usually converges with one or two extra function evaluations compared with ITER1 on well behaved functions like those found in ideal gas properties.  However, in multi-phase systems, ITER1 often fails to converge, and HYRID1 dramatically outperforms traditional bisection.  


\section{Considerations for Array Support}

\end{document}
